{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 227: Homework 3\n",
    "## Part (A): Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#import useful packages, all of them are important but not necessarily used in this code\n",
    "#enable inline plotting in Python Notebook\n",
    "#supress warnings\n",
    "\n",
    "%pylab inline\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import scipy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can access the network datasets exactly in the way you accessed them for HW 1. \n",
    "### Please DO NOT copy the datasets to your directory because it could result in 'timeout' errors during the submission. \n",
    "### Now that you are familiar with Gephi and its usage, you will explore some of its built-in tools to improve your visualizations. You will need your results from HW1. Only use the results obtained using NetworkX here. This can be done by adding your results on the node degrees, centralities etc. as attributes to the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this task, we will use the Filter tool of Gephi to threshold the available network data using various properties.\n",
    "### Visualise the Facebook, Enron-emails and Collaboration (Erdos) networks while applying the following thresholds. Make sure to have all the visualizations labelled with appropriate node labels. This is quite an open-ended question, as you have a lot of scope to make your visualizations better by trying out different layouts, colors etc. So, turn in the best visualization that you get in each case. You should attach an image (.png, .jpg) for each visualization here in the notebook itself. Also, make sure that it is well readable.\n",
    "### (1) Top ~20% nodes, thrsholded by Node Degree\n",
    "### (2) Top ~1% nodes, thrsholded by Node Degree\n",
    "### (3) Top ~20% nodes, thrsholded by Betweeness Centrality\n",
    "### (4) Top ~1% nodes, thrsholded by Betweeness Centrality\n",
    "\n",
    "### You may reproduce some of your HW 1 images in response to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\"Facebook top 20 percent degree\"\"\"\n",
    "![Facebook_top20_degree](facebook_20node.png)\n",
    "\"\"\"Facebook top 1 percent degree\"\"\"\n",
    "![Facebook_top1_degree](facebook_node1.png)\n",
    "\"\"\"Facebook top 20 percent betweeness\"\"\"\n",
    "![Facebook_top20_between](facebook_20between.png)\n",
    "\"\"\"Facebook top 1 percent betweeness\"\"\"\n",
    "![Facebook_top1_between](facebook_between1.png)\n",
    "\"\"\"Enron top 20 percent degree\"\"\"\n",
    "![enron_top20perc_degree](enron_degree20.png)\n",
    "\"\"\"Enron top 1 percent degree\"\"\"\n",
    "![enron_top1perc_degree](enron_degree1perc.png)\n",
    "\"\"\"Enron top 20 percent betweeness\"\"\"\n",
    "![Enron_top20_between](enron_between20.png)\n",
    "\"\"\"Enron top 1 percent betweeness\"\"\"\n",
    "![Enron_top1_between](enron_betweentop1.png)\n",
    "\"\"\"Erdos top 20 percent degree\"\"\"\n",
    "![Erdos_top20percdeg](erdos_degree20.png)\n",
    "\"\"\"Erdos top 1 percent degree\"\"\"\n",
    "![Erdos_top20percdeg](erdos_degree1.png)\n",
    "\"\"\"Erdos top 20 percent betweeness\"\"\"\n",
    "![Erdos_top20percdeg](erdos_between20.png)\n",
    "\"\"\"Erdos top 1 percent betweeness\"\"\"\n",
    "![Erdos_top20percdeg](erdos_betweentop1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (B): Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this task, we will try to find communities in the given networks and learn more about them. There are libraries that can be used with NetworkX for community detection (http://perso.crans.org/aynaud/communities/). In addition to NetworkX, we will also use igraph library in this task for community detection purposes. If not already available, install the following required packages and have a look at their documentations to gain some familiarity with them. More information on community detection can also be found here: https://arxiv.org/abs/0906.0612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-louvain\n",
    "#!pip install python-igraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are multiple algorithms to detect communities. One of the commonly used algorithms is Louvain algorithm. The method is a greedy optimization method that attempts to optimize the \"modularity\" of a partition of the network. The 'community' library uses Louvain algorithm, and hence we get partitions based on optimized modularity. Implement a python code using the 'community' library to find communities in the Citations network and the Collaboration network (Erdos). Write your code in the next cell and visualize your community detection results in Gephi for both the networks. Label the nodes in the visualization properly. Use the largest connected components, if required. Include the images (.jpg, .png) of your visualizations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import urllib\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"../data\")\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "try:\n",
    "    os.makedirs(\"../data/citnet\")\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "        \n",
    "urllib.request.urlretrieve(\"http://snap.stanford.edu/data/cit-HepTh.txt.gz\",\"../data/citnet/citnet_combined.txt.gz\")\n",
    "\n",
    "import gzip\n",
    "# datapath = \"../../../datasets/ece227-fa19-public/\"\n",
    "inF = gzip.GzipFile(\"../data/citnet/citnet_combined.txt.gz\", 'rb')\n",
    "s = inF.read()\n",
    "inF.close()\n",
    "\n",
    "outF = open(\"../data/citnet/citnet_combined.txt\", 'wb')\n",
    "outF.write(s)\n",
    "outF.close()\n",
    "\n",
    "file_name = \"../data/citnet/citnet_combined.txt\"\n",
    "#convert the information in the text file into a graph, find no. of edges & nodes in the graph\n",
    "\n",
    "g3=nx.read_edgelist(file_name,create_using=nx.Graph(),nodetype=int)\n",
    "node, edge=g3.order(),g3.size()\n",
    "print(nx.info(g3))\n",
    "\n",
    "def deg_dict_generator(G):\n",
    "    dic = {}\n",
    "    for node in G.nodes():\n",
    "        dic[node] = G.degree(node)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "\n",
    "deg_dictionary3 = deg_dict_generator(g3)\n",
    "\n",
    "nx.set_node_attributes(g3,deg_dictionary3,'degree')\n",
    "\n",
    "\n",
    "partition = community.best_partition(g3,deg_dictionary3)\n",
    "\n",
    "nx.set_node_attributes(g3,partition,'community')\n",
    "nx.write_gml(g3,'citnet.gml')\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"data/erdos\")\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "        \n",
    "# IF THE FOLLOWING FAILS DOWNLOAD THE PAGE MANUALLY...\n",
    "\n",
    "#urllib.request.urlretrieve(\"https://files.oakland.edu/users/grossman/enp/Erdos1.html\", \"data/erdos/Erdos1.html\") \n",
    "\n",
    "g4 = nx.Graph()\n",
    "dict_authors = {}\n",
    "dict_authors['Paul Erdos'] = 0\n",
    "g4.add_node(0)\n",
    "g4.nodes[0]['author'] = 'Paul Erdos'\n",
    "\n",
    "# add the authors with Erdos number 1 and 2 from file\n",
    "line_count = 1\n",
    "skip_line = 24\n",
    "skip_space = 1\n",
    "\n",
    "is_new = False\n",
    "author = \"\"\n",
    "coauthor = \"\"\n",
    "index = 1\n",
    "ind_author = 1\n",
    "ind_coauthor = 1\n",
    "\n",
    "def parseLine(l, start):\n",
    "    end = start\n",
    "    while end < len(l) - 1 and not (l[end] == ' ' and l[end + 1] == ' '):\n",
    "        end += 1\n",
    "    return l[start:end]\n",
    "\n",
    "def addAuthor(auth, ind):\n",
    "    if auth in dict_authors:\n",
    "        return ind\n",
    "    dict_authors[auth] = ind\n",
    "    return ind + 1\n",
    "\n",
    "for l in open(\"C:/Users/brand/Desktop/Erdos1.html\"):    \n",
    "    if line_count >= skip_line:\n",
    "        if l == '\\n':\n",
    "            is_new = True\n",
    "        elif is_new:\n",
    "            author = parseLine(l, 0)\n",
    "            index = addAuthor(author, index)\n",
    "            ind_author = dict_authors[author]\n",
    "            g4.add_edge(0, ind_author)\n",
    "            g4.node[ind_author]['author'] = author\n",
    "            is_new = False\n",
    "        elif l == '</pre>':\n",
    "            break\n",
    "        else:\n",
    "            coauthor = parseLine(l, skip_space)\n",
    "            index = addAuthor(coauthor, index)\n",
    "            ind_coauthor = dict_authors[coauthor]\n",
    "            g4.add_edge(ind_author, ind_coauthor)\n",
    "            g4.node[ind_coauthor]['author'] = coauthor\n",
    "    line_count += 1\n",
    "\n",
    "print(nx.info(g4))\n",
    "\n",
    "def deg_dict_generator(G):\n",
    "\"\"\"\"\"\"    dic = {}\n",
    "    for node in G.nodes():\n",
    "        dic[node] = G.degree(node)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "\n",
    "deg_dictionary4 = deg_dict_generator(g4)\n",
    "\n",
    "nx.set_node_attributes(g4,deg_dictionary4,'degree')\n",
    "\n",
    "\n",
    "partition = community.best_partition(g4,deg_dictionary4)\n",
    "\n",
    "nx.set_node_attributes(g4,partition,'community')\n",
    "nx.write_gml(g4,'erdosnethw2.gml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Citations Network top 3 Communites using Louvain\"\"\"\n",
    "\n",
    "![Citations Network Community top 3](citationnetwork_community_top3_COMMUNITY.png)\n",
    "\n",
    "\"\"\"Erdos Network top 3 Communites using Louvain\"\"\"\n",
    "![Erdos Network Community top 3](erdosnetwork_top4communities_COMMUNITY.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to 'community' library, 'igraph' has more flexibilty to detect communities. Igraph allows the user to partition the network into the number of communities that the user wishes. Obviously this number is bounded. Now, you will use this feature to divide the given network into '5' communities using 'igraph' and observe the results. Write a python code to implement the above task for the Citation and the Erdos networks. Remember that unlike 'community', igraph provides multiple approaches to community detection, the most obvious approach being the greedy one because it optimizes modularity. Visualize your community detection results in Gephi for both the networks. Label the nodes in the visualization properly. Use largest connected components if required. Use different colors for nodes in every community. Include the images(.jpg, .png) of your visualizations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "g = igraph.Graph.from_networkx(g3)\n",
    "GG = g.clusters().giant()\n",
    "com = GG.community_leading_eigenvector(clusters=5)\n",
    "\n",
    "membership = com.membership\n",
    "GG.vs[\"membership\"] = membership\n",
    "\n",
    "GG.write_gml('citationnet.gml')\n",
    "\n",
    "\n",
    "G = igraph.Graph.from_networkx(g4)\n",
    "G.delete_vertices(1)\n",
    "comG = G.community_leading_eigenvector(clusters=5)\n",
    "\n",
    "membershipG = comG.membership\n",
    "G.vs[\"membership\"] = membershipG\n",
    "\n",
    "degs = G.degree()\n",
    "G.vs['degree'] = degs\n",
    "\n",
    "G.write_gml('erdosnetigraph.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Top 5 citation net communities using Igraph\"\"\"\n",
    "![Citations Network Igraph top 5](citationnet_igraph_5communities.png)\n",
    "\n",
    "\"\"\"Top 5 Erdos net communities using Igraph\"\"\"\n",
    "![Erdos Network Igraph top 5](erdosnetigraph_5communitesreal_IGRAPH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that you have detected the communities, you will analyze your results further. This task is only for the Collaboration network (Erdos). Use results from the communtiy detection performed using 'community'. Sort the communities and get the largest 5 communities. For each of these 5 communities, get 3 nodes with the highest node degree. So you will get 3 authors per community, for 5 communities. Now, find out the areas of research of each of these authors and enlist them. Further, observe if there is any reason for these 3 authors to be in same community (do this for each community). State this reason in brief. Write all of your results in the next cell. Also include any other interesting results that you may observe during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Community 1:\n",
    "    \n",
    "    Paul Erdos- Mathematics\n",
    "    Ralph Philip Boas Jr- Mathematics/ complex analysis\n",
    "    Samuel James Taylor- MAthematics/Brownian motion and Fractals \n",
    "    \n",
    "    Each author is active in abstract non-applied mathematics. \n",
    "\n",
    "Community 2-\n",
    "    Noga Alon- Mathematics/combinatorics and graph theory\n",
    "    Zsolt Tuza- Mathematics/graph theory\n",
    "    Douglas Brent- Graph theory\n",
    "    \n",
    "    The three authors are all involved in graph theory and combinatorics.\n",
    "\n",
    "Community 3-\n",
    "    Frank Harary-Graph theory\n",
    "    Pavol Hell-Math and computer science/algorithms and computational combinatorics \n",
    "    Douglas Robert Stinson- Math/cryptography \n",
    "    \n",
    "    Again, the authors share a common interest in the application of mathematics to computer science.\n",
    "\n",
    "Community 4-\n",
    "    Stephen Travis Hedentiemi- math and computer science/ graphs and geneology\n",
    "    Michael Anthony Henning- math/geneology \n",
    "    Wayne Dean Goddard- Geneology and graph theory\n",
    "   \n",
    "   Here the relevant authors are involved in geneology and graph theory \n",
    "    \n",
    "Community 5-\n",
    "    Charles Joseph Colbourn- cs and math focusing on graph algorithms and combinational design\n",
    "    Saharon Shelah- Mathematical Logic\n",
    "    Peter Salamon- Thermodynamics,optimization and biomathematics \n",
    "    \n",
    "    This is the one community where there is some slight heterogeneity among the author's areas of research, but they are still focused primarily on applied mathematics. \n",
    "\n",
    "It is clear that the individuals in each community study a similar field of mathematics. The papers published in the five communities demonstrate a homogeneity of focus. The one exception could be group 1, but the three ares of focus among the three authors is still pure nonapplied mathematics.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (C) COVID19 Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this homework we are going to gain some basic familiarity with a Susceptible-Exposed-Infectious-Recovered/deceased (SEIR)-based simulation model for estimating the number of infections, hospitalizations, and deaths due to COVID19. The model that we are going to use is a version of SEIR model used for the https://covid19-projections.com/ that was one of the very successful projection websites for COVID19. You can download (clone) the simulator from here https://github.com/youyanggu/yyg-seir-simulator and upload the *necessary* files to the datahub (do not upload the entire source code and parameters and try to put them outside your submission folder to avoid submission error).\n",
    "### Using this simulator answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. In one paragraph explain how the SEIR model works and find and explain the part of the library that has to do with the core idea of the SEIR model that calculates the number of deaths (the second part has extra points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The SEIR model categorizes a population's individuals into one of four groups: susceptible,exposed,infected or recovered. At any\n",
    "time, an individual may move from one of these four states to another. The likelihood that an individual moves from susceptible\n",
    "to infected depends on paramters like the contraction rate, social distancing measures, and other environemntal factors.\n",
    "Similarly, the probability an infected individual succumbs to the illness depends on the mortality rate of the virus. The\n",
    "model used by Yang seeks to learn these parameters by grid search, or by running the model on the observed data under various \n",
    "hyperparameters and giving higher weight to the parameters that minimize the loss function better. These parameters are learned\n",
    "by minimizing a loss function on the JHU daily death dataset. Once these paramaters are learned, the parameters can be fed into \n",
    "the SEIR model, which simulates how the virus spreads and the rate at which the population succumbs to the virus based on the \n",
    "learned parameters. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using this simulation model whose parameters are last updated in October 2020, predict the total number of deaths by the end of 2020 and compare the result with the actual number of deaths in the US (you can use John Hopkins website for the reference: https://coronavirus.jhu.edu/us-map). Is the model over-estimating or under-estimating the number of deaths? In your opinion why the model predicts the number of deaths differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimated deaths ending on 12/31/2020:        328,274 deaths\n",
    "Actual deaths on 12/31/2020 according to CDC: 342,199 deaths\n",
    "\n",
    "The model under-estimates the death total but only by 4 percent. I am not qualified to weigh in on the matter, but I would \n",
    "propose the model does not properly adjust for the seasonality of the virus in the winter months, or underestimated the \n",
    "lockdown's fatigue effects and the population's growing distaste of lockdown and social distancing measures. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. If the initial R_0 of the model was increased by 20% how many more deaths would have occured compared to the model's original prediction in the US? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adjusting R_0 to 2.46 from 2.26 leads to:\n",
    "\n",
    "Estimated deaths under new R0 - 310,185\n",
    "Original Estimated deaths - 224,263\n",
    "\n",
    "Difference of 85,922 deaths \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Answer the above question for the case where the openning date in the US was increased by 10 days and decreased by 10 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changing the reopening date to 4/30/20 and 5/20/2020 from 5/10/2020:\n",
    "\n",
    "Estimated deaths with 4/30 opening date- 245,311\n",
    "Estimated deaths with 5/10 opening date- 224,263\n",
    "Estimated deaths with 5/20 opening date- 205,355\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. According to this model, how many fewer deaths would have occurred if the social distancing in the US had started just 2 days earlier? How about 4, 7, and 10 days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "2 days-177,802 total estimated deaths ---> 46,461 estimated fewer deaths\n",
    "4 days- 138,615 total estimated deaths ---> 85,648 estimated fewer deaths\n",
    "7 days- 92,880 total estimated deaths ---> 131,383 estimated fewer deaths\n",
    "10 days- 61,091 total estimated deaths ---> 163,172 estimated fewer deaths\n",
    "\n",
    "I am subtracting the models estimate under the best parameters, not the actual death total, from the new social\n",
    "distancing dates\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
